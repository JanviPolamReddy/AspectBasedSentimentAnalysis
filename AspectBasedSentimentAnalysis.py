# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/176axmvFMIGhBvQuDziGRKgsLkVZ0Irt7
"""

import xml.etree.ElementTree as ET
from transformers import BertForTokenClassification
from torch.utils.data import TensorDataset, DataLoader
from transformers import BertTokenizer, BertModel
import torch

def extract_and_preprocess_train_data(xml_file, tokenizer):
    tree = ET.parse(xml_file)
    root = tree.getroot()

    texts = []
    aspect_terms = []
    aspect_sentiments = []

    for sentence in root.findall('sentence'):
        text = sentence.find('text').text.strip()
        texts.append(text)

        aspect_terms_sentence = []
        aspect_sentiments_sentence = []
        aspect_terms_xml = sentence.find('aspectTerms')
        if aspect_terms_xml is not None:
            for aspect_term in aspect_terms_xml.findall('aspectTerm'):
                aspect_terms_sentence.append(aspect_term.attrib['term'])
                aspect_sentiments_sentence.append(aspect_term.attrib['polarity'])
        aspect_terms.append(aspect_terms_sentence)
        aspect_sentiments.append(aspect_sentiments_sentence)

    input_ids = []
    attention_masks = []
    token_type_ids = []
    labels = []

    for text, terms, sentiments in zip(texts, aspect_terms, aspect_sentiments):
        encoded_dict = tokenizer.encode_plus(
                            text,
                            add_special_tokens = True,
                            max_length = 128,
                            padding='max_length',
                            pad_to_max_length = True,
                            return_attention_mask = True,
                            return_tensors = 'pt',
                       )
        input_ids.append(encoded_dict['input_ids'])
        attention_masks.append(encoded_dict['attention_mask'])
        token_type_ids.append(encoded_dict['token_type_ids'])

        labels_sentence = []
        for token in tokenizer.tokenize(text):
            if token in terms:
                idx = terms.index(token)
                if sentiments[idx] == 'positive':
                    labels_sentence.append(2)
                elif sentiments[idx] == 'negative':
                    labels_sentence.append(0)
                else:
                    labels_sentence.append(1)
            else:
                labels_sentence.append(-100)

        while len(labels_sentence) < len(encoded_dict['input_ids'][0]):
            labels_sentence.append(-100)

        labels.append(labels_sentence)

    input_ids = torch.cat(input_ids, dim=0)
    attention_masks = torch.cat(attention_masks, dim=0)
    token_type_ids = torch.cat(token_type_ids, dim=0)
    labels = torch.tensor(labels)

    return input_ids, attention_masks, token_type_ids, labels

def extract_and_preprocess_test_data(xml_file, tokenizer):
    tree = ET.parse(xml_file)
    root = tree.getroot()

    input_ids_list = []
    attention_masks_list = []

    for sentence in root.findall('sentence'):
        text = sentence.find('text').text.strip()
        encoded_dict = tokenizer.encode_plus(
                            text,
                            add_special_tokens = True,
                            max_length = 128,
                            padding='max_length',
                            pad_to_max_length = True,
                            return_attention_mask = True,
                            return_tensors = 'pt',
                       )
        input_ids_list.append(encoded_dict['input_ids'])
        attention_masks_list.append(encoded_dict['attention_mask'])

    input_ids = torch.cat(input_ids_list, dim=0)
    attention_masks = torch.cat(attention_masks_list, dim=0)

    return input_ids, attention_masks

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

train_input_ids, train_attention_masks, train_token_type_ids, train_labels = extract_and_preprocess_train_data('Laptops_Train.xml', tokenizer)

test_input_ids, test_attention_masks = extract_and_preprocess_test_data('Laptops_Test_Data_PhaseA.xml', tokenizer)

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

test_dataset = TensorDataset(test_input_ids, test_attention_masks)
test_dataloader = DataLoader(test_dataset, batch_size=32)

predictions = []
label_map = {0: "negative", 1: "neutral", 2: "positive", 3: "conflict"}
predicted_labels = []

model.eval()

with torch.no_grad():
    for batch in test_dataloader:
        input_ids, attention_masks = batch

        outputs = model(input_ids=input_ids, attention_mask=attention_masks)
        last_hidden_state = outputs.last_hidden_state
        predicted_label_indices = (last_hidden_state.argmax(dim=-1))

        predictions.extend(predicted_label_indices.tolist())

        for index_list in predicted_label_indices:
            batch_predicted_labels = [label_map[index.item()] for index in index_list]
            predicted_labels.extend(batch_predicted_labels)

print("Predicted Labels:", predicted_labels)